{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c16cef-2fde-4d31-878e-c31d1e177ebf",
   "metadata": {},
   "source": [
    "# Demonstration of the Observable-Tunable Expectation Value Sampler Quantum Generative Model (OT-EVS) on the Fashion-MNIST Dataset\n",
    "### This notebook produces similar results as in Section V.D. of the article \"Shadow-Frugal Expectation-Value-Sampling Variational Quantum Generative Model\" (arXiv:2412.17039).\n",
    "### We use the 32-dimensional compressed Fashion-MNIST dataset. The pretrained autoencoder and compressed data are located in the folder 'data/'. New autoencoders can be trained in the notebook there as well.\n",
    "### Below we demonstrate the training of OT-EVS with the conventional measurements. Users may apply the classical shadows measurements easily based on other notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363a7cc-5b00-4c7f-bb31-855bf20cdfcf",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a92d6-0051-4d27-b708-96f58ed948d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from itertools import product\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import PRNGKeyArray\n",
    "from jax.tree_util import tree_map\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import tensorcircuit as tc\n",
    "\n",
    "import faiss\n",
    "from scipy.special import digamma\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fc846-f996-4d31-8a73-632e43bb5627",
   "metadata": {},
   "source": [
    "## Experiment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc009c8-40f2-4514-8f1a-1cece2e56ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # critic architecture\n",
    "    'critic_layer_size': 512,   # width of MLP hidden layers in the critic\n",
    "    'critic_depth': 4,  # depth of MLP hidden layers in the critic\n",
    "    'n_critic': 5,  # how many times to update the critic before updating the generator\n",
    "\n",
    "    # generator architecture\n",
    "    'latent_dim': 2,   # dimension of input Gaussian random variables\n",
    "    'data_dim': 32,  # dimension of output, we use 8 dimensional compressed MNIST\n",
    "    'original_data_dim': 784, # the dimension of original MNIST\n",
    "    'nq': 8,  # number of qubits\n",
    "    'nl': 2,   # number of circuit layers\n",
    "    'k': 1,   # locality of observables\n",
    "    'n_shots': 1024,   # number of shots per observable\n",
    "\n",
    "    # learning and decay rates for the generator (circuit part)\n",
    "    'lr_gq': 1e-2, \n",
    "    'b1_gq': 0.9,\n",
    "    'b2_gq': 0.9,\n",
    "\n",
    "    # learning and decay rates for the generator (observable part)\n",
    "    'lr_gl': 1e-3, \n",
    "    'b1_gl': 0,\n",
    "    'b2_gl': 0.99,\n",
    "\n",
    "    # learning and decay rates for the critic\n",
    "    'lr_c': 1e-3,\n",
    "    'b1_c': 0.5,\n",
    "    'b2_c': 0.5,\n",
    "\n",
    "    'lambda_gp': 1,  # scalar in front of the gradient penalty term \n",
    "    'batch_size': 256,  # batch size\n",
    "\n",
    "    'n_iter': 5000,  # how many training iterations to use\n",
    "    'eval_freq': 200,   # how often to estimate the KLD\n",
    "    'eval_size': 2048   # how many samples and training data to use to estimate the KLD\n",
    "}\n",
    "\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481ab15-922b-434d-ba25-d79856010e71",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7b16a-dbeb-4527-8495-1fa36427e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tc.set_backend('jax')\n",
    "\n",
    "def get_all_k_local_observables(nq, k):\n",
    "    '''\n",
    "    The observables\n",
    "    '''\n",
    "    all_tuples = product([0, 1, 2, 3], repeat=nq)\n",
    "    valid_tuples = [t for t in all_tuples if (sum(1 for x in t if x == 0) >= nq - k and sum(1 for x in t if x == 0) < nq)]\n",
    "    \n",
    "    return jnp.array(valid_tuples)\n",
    "\n",
    "\n",
    "def get_circuit(nq, nl, inputs, weights):\n",
    "    '''\n",
    "    The circuit\n",
    "    '''\n",
    "    circuit = tc.Circuit(nq)\n",
    "    for l in range(nl):\n",
    "        for i in range(nq):\n",
    "            circuit.rx(i, theta=inputs[l])\n",
    "            circuit.ry(i, theta=weights[l,i])\n",
    "        for i in range(0,nq-1):\n",
    "            circuit.cnot(i, i+1)\n",
    "            circuit.ry(i+1, theta=weights[l, nq+i])\n",
    "            circuit.cnot(i, i+1)\n",
    "    \n",
    "    return circuit \n",
    "\n",
    "\n",
    "class GeneratorQuantum(eqx.Module):\n",
    "    nq: int = eqx.field(static=True)\n",
    "    nl: int = eqx.field(static=True)\n",
    "    k: int = eqx.field(static=True)\n",
    "    weights: jax.Array\n",
    "    \n",
    "    @K.jit\n",
    "    def evaluate_circuit(self, inputs, observable):\n",
    "        circuit = get_circuit(self.nq, self.nl, inputs, self.weights)\n",
    "        return tc.templates.measurements.parameterized_measurements(circuit, observable, onehot=True)\n",
    "\n",
    "    ### Call this function when using the classical shadows method\n",
    "    def get_2k_values(self, x):\n",
    "        all_2k_observables = get_all_k_local_observables(self.nq, int(min(2 * self.k, self.nq)))\n",
    "        return K.vmap(self.evaluate_circuit, vectorized_argnums=1)(x, all_2k_observables)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        all_observables = get_all_k_local_observables(self.nq, self.k)\n",
    "\n",
    "        return K.vmap(self.evaluate_circuit, vectorized_argnums=1)(x, all_observables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678d6dd-e986-4848-9e8f-d2b6c319b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorLinear(eqx.Module):\n",
    "    ''' \n",
    "    The generator (observable part)\n",
    "    '''\n",
    "    model: eqx.Module\n",
    "    \n",
    "    def __init__(self, n_obs, data_dim, key):\n",
    "        super(GeneratorLinear, self).__init__()\n",
    "        \n",
    "        self.model = eqx.nn.Linear(n_obs, data_dim, key=key)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c332e0b-1fe7-4cfd-b304-ecd11e1caeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(eqx.Module):\n",
    "    '''\n",
    "    The critic\n",
    "    '''\n",
    "    layers:list\n",
    "    \n",
    "    def __init__(self, data_dim, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(data_dim, 512, key=key1), \n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(512, 512, key=key2),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(512, 512, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(512, 1, key=key4),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6efd3-197f-479c-9bf8-f456779662f7",
   "metadata": {},
   "source": [
    "## KLD Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943397ab-9bab-4ed6-9b82-ffcd0a6270c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kld_estimator(s1, s2):\n",
    "    # equation 25 of the reference paper\n",
    "    s1, s2 = np.array(s1), np.array(s2)\n",
    "    n, m = len(s1), len(s2)\n",
    "    d = int(s1.shape[1])\n",
    "\n",
    "    #res = faiss.StandardGpuResources()\n",
    "\n",
    "    index_s1=faiss.IndexFlatL2(d)\n",
    "    #index_s1=faiss.index_cpu_to_gpu(res,0,index_s1)\n",
    "    index_s1.add(s1)\n",
    "\n",
    "    index_s2=faiss.IndexFlatL2(d)\n",
    "    #index_s2=faiss.index_cpu_to_gpu(res,0,index_s2)\n",
    "    index_s2.add(s2)\n",
    "\n",
    "    fulldist1 = np.sqrt(index_s1.search(s1, n)[0])\n",
    "    fulldist2 = np.sqrt(index_s2.search(s1, m)[0])\n",
    "\n",
    "    rhoi=fulldist1[::,1].reshape(-1)\n",
    "    nui=fulldist2[::,0].reshape(-1)\n",
    "\n",
    "    epsilon=np.maximum(rhoi, nui)\n",
    "    arg=np.where(rhoi>=nui, 0, 1)\n",
    "\n",
    "    li = np.array([np.searchsorted(fulldist1[i], epsilon[i], side='right') for i in range(m)]) - 1\n",
    "    ki = np.array([np.searchsorted(fulldist2[i], epsilon[i], side='right') for i in range(n)])\n",
    "\n",
    "\n",
    "    return np.mean(digamma(li)-digamma(ki)) + np.log(m / (n - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3525c-d9e2-4e80-b8d5-71b5db8c630e",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cfc8d-62e9-46ec-b77b-f551be228545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, training_data_path, seed_initial):\n",
    "    ############################# Data loader #####################################\n",
    "    def dataloader(data, batch_size, *, key):\n",
    "        dataset_size = data.shape[0]\n",
    "        indices = jnp.arange(dataset_size)\n",
    "        while True:\n",
    "            key, subkey = jax.random.split(key, 2)\n",
    "            perm = jax.random.permutation(subkey, indices)\n",
    "            start = 0\n",
    "            end = batch_size\n",
    "            while end < dataset_size:\n",
    "                batch_perm = perm[start:end]\n",
    "                yield data[batch_perm]\n",
    "                start = end\n",
    "                end = start + batch_size \n",
    "    # Yield a batch of training data from the training set\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from dataloader\n",
    "\n",
    "    ############################ Add shot noise (conventional method) to ideal outputs ############################\n",
    "    @jax.jit\n",
    "    def add_sampling_error(exact, n_shots, key):\n",
    "        p = jnp.clip((1 - exact) / 2, 0, 1)\n",
    "        mean = n_shots * p\n",
    "        std = jnp.sqrt(jnp.clip(n_shots * p * (1 - p), min=1e-16))\n",
    "        return 1 - 2 * jnp.clip((jax.random.normal(key) * std + mean) / n_shots, 0, 1)\n",
    "        \n",
    "    ###################################### One iteration during training  ######################################\n",
    "    @eqx.filter_jit\n",
    "    def train_step(generator_quantum_params, generator_linear_params, critic_params, generator_quantum_opt_state, generator_linear_opt_state, critic_opt_state, key):\n",
    "\n",
    "        # The block below contains subroutines  for a training step. \n",
    "        ############################### Conventional Method ##############################\n",
    "        # Evaluate the generator (circuit part)\n",
    "        @eqx.filter_value_and_grad(has_aux=False)\n",
    "        def compute_grads_generator_quantum(generator_quantum_params, generator_linear_params, critic_params, z, keys):\n",
    "            generator_quantum = eqx.combine(generator_quantum_params, generator_quantum_static)\n",
    "            fake_batch_intermediate = jax.vmap(generator_quantum, in_axes=0, out_axes=0)(z)\n",
    "            fake_batch_sampled = jax.vmap(jax.vmap(add_sampling_error, in_axes=(0,None,0)), in_axes=(0, None, 0))(fake_batch_intermediate, config.n_shots, keys)\n",
    "            generator_linear = eqx.combine(generator_linear_params, generator_linear_static)\n",
    "            fake_batch = jax.vmap(generator_linear, in_axes=0, out_axes=0)(fake_batch_sampled)\n",
    "            critic = eqx.combine(critic_params, critic_static)\n",
    "            fake_value = jax.vmap(critic, in_axes=0, out_axes=0)(fake_batch)\n",
    "            loss = -fake_value.mean()\n",
    "\n",
    "            return loss\n",
    "        \n",
    "        # Evaluate the generator (observable part)\n",
    "        @eqx.filter_value_and_grad(has_aux=False)\n",
    "        def compute_grads_generator_linear(generator_linear_params, generator_quantum_params, critic_params, z, keys):\n",
    "            generator_quantum = eqx.combine(generator_quantum_params, generator_quantum_static)\n",
    "            fake_batch_intermediate = jax.vmap(generator_quantum, in_axes=0, out_axes=0)(z)\n",
    "            fake_batch_sampled = jax.vmap(jax.vmap(add_sampling_error, in_axes=(0,None,0)), in_axes=(0, None, 0))(fake_batch_intermediate, config.n_shots, keys)\n",
    "            generator_linear = eqx.combine(generator_linear_params, generator_linear_static)\n",
    "            fake_batch = jax.vmap(generator_linear, in_axes=0, out_axes=0)(fake_batch_sampled)\n",
    "            critic = eqx.combine(critic_params, critic_static)\n",
    "            fake_value = jax.vmap(critic, in_axes=0, out_axes=0)(fake_batch)\n",
    "            loss = -fake_value.mean()\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Subroutine for evaluating the critic\n",
    "        @eqx.filter_vmap(in_axes=(0, None))\n",
    "        @eqx.filter_grad(has_aux=False)\n",
    "        def critic_forward(input_data, critic):\n",
    "            value = critic(input_data)\n",
    "            return value[0]\n",
    "\n",
    "        # Evaluate the critic\n",
    "        @eqx.filter_value_and_grad(has_aux=False)\n",
    "        def compute_grads_critic(critic_params, generator_quantum_params, generator_linear_params, real_batch, z, key, keys):\n",
    "            generator_quantum = eqx.combine(generator_quantum_params, generator_quantum_static)\n",
    "            fake_batch_intermediate = jax.vmap(generator_quantum, in_axes=0, out_axes=0)(z)\n",
    "            fake_batch_sampled = jax.vmap(jax.vmap(add_sampling_error, in_axes=(0,None,0)), in_axes=(0, None, 0))(fake_batch_intermediate, config.n_shots, keys)\n",
    "            generator_linear = eqx.combine(generator_linear_params, generator_linear_static)\n",
    "            fake_batch = jax.vmap(generator_linear, in_axes=0, out_axes=0)(fake_batch_sampled)\n",
    "            critic = eqx.combine(critic_params, critic_static)\n",
    "            fake_value = jax.vmap(critic, in_axes=0, out_axes=0)(fake_batch)  # first term of the critic loss\n",
    "            real_value = jax.vmap(critic, in_axes=0, out_axes=0)(real_batch)  # second term of the critic loss\n",
    "             \n",
    "            epsilon = jax.random.uniform(key, shape=(config.batch_size, 1), minval=0, maxval=1)\n",
    "            data_mix = real_batch * epsilon + fake_batch * (1 - epsilon) \n",
    "            grads = critic_forward(data_mix, critic)\n",
    "            grad_norm = jnp.linalg.norm(grads, axis=1)\n",
    "            gradient_penalty = jnp.mean((grad_norm - 1) ** 2)  # gradient penalty term \n",
    "            \n",
    "            loss = -jnp.mean(real_value) + jnp.mean(fake_value) + config.lambda_gp * gradient_penalty\n",
    "\n",
    "            return loss\n",
    "            \n",
    "        ### The block below is what differs for the three training algorithms. Comment and Uncomment the blocks to switch algorithms\n",
    "\n",
    "        ########################################### Asynchronous version ########################################################\n",
    "        '''\n",
    "        for _, real_batch in zip(range(config.n_critic), infinite_trainloader()):\n",
    "            key, subkey, subsubkey, subsubsubkey = jax.random.split(key, 4)\n",
    "            z = jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "            keys = jax.random.split(subsubsubkey, n_obs * config.batch_size).reshape(config.batch_size, n_obs, 2)\n",
    "            \n",
    "            loss_critic, grads = compute_grads_critic(critic_params, generator_quantum_params, generator_linear_params, real_batch, z, subsubkey, keys)\n",
    "            updates, critic_opt_state = tx_c.update(grads, critic_opt_state)\n",
    "            critic_params = eqx.apply_updates(critic_params, updates)            \n",
    "        \n",
    "        key, subkey, subsubkey = jax.random.split(key, 3)\n",
    "        z = jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "        keys = jax.random.split(subsubkey, n_obs * config.batch_size).reshape(config.batch_size, n_obs, 2)\n",
    "        \n",
    "        for _ in range(config.n_critic):\n",
    "            loss_generator_linear, grads = compute_grads_generator_linear(generator_linear_params, generator_quantum_params, critic_params, z, keys)\n",
    "            updates, generator_linear_opt_state = tx_gl.update(grads, generator_linear_opt_state)\n",
    "            generator_linear_params = eqx.apply_updates(generator_linear_params, updates) \n",
    "        \n",
    "        loss_generator_quantum, grads = compute_grads_generator_quantum(generator_quantum_params, generator_linear_params, critic_params, z, keys)\n",
    "        updates, generator_quantum_opt_state = tx_gq.update(grads, generator_quantum_opt_state)\n",
    "        generator_quantum_params = eqx.apply_updates(generator_quantum_params, updates) \n",
    "        '''\n",
    "        ########################################### Decoupled version ##########################################################\n",
    "        '''\n",
    "        for _, real_batch in zip(range(config.n_critic), infinite_trainloader()):\n",
    "            key, subkey, subsubkey, subsubsubkey = jax.random.split(key, 4)\n",
    "            z = jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "            keys = jax.random.split(subsubsubkey, n_obs * config.batch_size).reshape(config.batch_size, n_obs, 2)\n",
    "            \n",
    "            loss_critic, grads = compute_grads_critic(critic_params, generator_quantum_params, generator_linear_params, real_batch, z, subsubkey, keys)\n",
    "            updates, critic_opt_state = tx_c.update(grads, critic_opt_state)\n",
    "            critic_params = eqx.apply_updates(critic_params, updates)            \n",
    "\n",
    "            loss_generator_linear, grads = compute_grads_generator_linear(generator_linear_params, generator_quantum_params, critic_params, z, keys)\n",
    "            updates, generator_linear_opt_state = tx_gl.update(grads, generator_linear_opt_state)\n",
    "            generator_linear_params = eqx.apply_updates(generator_linear_params, updates) \n",
    "        \n",
    "        key, subkey, subsubkey = jax.random.split(key, 3)\n",
    "        z = jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "        keys = jax.random.split(subsubkey, n_obs * config.batch_size).reshape(config.batch_size, n_obs, 2)\n",
    "        \n",
    "        loss_generator_quantum, grads = compute_grads_generator_quantum(generator_quantum_params, generator_linear_params, critic_params, z, keys)\n",
    "        updates, generator_quantum_opt_state = tx_gq.update(grads, generator_quantum_opt_state)\n",
    "        generator_quantum_params = eqx.apply_updates(generator_quantum_params, updates) \n",
    "        '''\n",
    "\n",
    "        ########################################### Joint version ##############################################################  \n",
    "\n",
    "        for _, real_batch in zip(range(config.n_critic), infinite_trainloader()):\n",
    "            key, subkey, subsubkey, subsubsubkey = jax.random.split(key, 4)\n",
    "            z = jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "            keys = jax.random.split(subsubsubkey, n_obs * config.batch_size).reshape(config.batch_size, n_obs, 2)\n",
    "            \n",
    "            loss_critic, grads = compute_grads_critic(critic_params, generator_quantum_params, generator_linear_params, real_batch, z, subsubkey, keys)\n",
    "            updates, critic_opt_state = tx_c.update(grads, critic_opt_state)\n",
    "            critic_params = eqx.apply_updates(critic_params, updates)            \n",
    "        \n",
    "        key, subkey, subsubkey = jax.random.split(key, 3)\n",
    "        z= jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "        keys = jax.random.split(subsubkey, n_obs * config.batch_size).reshape(config.batch_size, n_obs, 2)\n",
    "        loss_generator_linear, grads_linear = compute_grads_generator_linear(generator_linear_params, generator_quantum_params, critic_params, z, keys)\n",
    "        loss_generator_quantum, grads_quantum = compute_grads_generator_quantum(generator_quantum_params, generator_linear_params, critic_params, z, keys)\n",
    "        updates_linear, generator_linear_opt_state = tx_gl.update(grads_linear, generator_linear_opt_state)\n",
    "        generator_linear_params = eqx.apply_updates(generator_linear_params, updates_linear) \n",
    "        updates_quantum, generator_quantum_opt_state = tx_gq.update(grads_quantum, generator_quantum_opt_state)\n",
    "        generator_quantum_params = eqx.apply_updates(generator_quantum_params, updates_quantum) \n",
    "        \n",
    "        return generator_quantum_params, generator_linear_params, critic_params, generator_quantum_opt_state, generator_linear_opt_state, critic_opt_state, loss_generator_quantum, loss_generator_linear, loss_critic, key\n",
    "                \n",
    "\n",
    "    # Generate fake samples to evaluate the model\n",
    "    @eqx.filter_jit\n",
    "    def evaluate_fake(generator_quantum_params, generator_linear_params, key):\n",
    "        z = jax.random.uniform(key, shape=(config.eval_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)  # now a full-size sample, not just a batch.\n",
    "        \n",
    "        generator_quantum = eqx.combine(generator_quantum_params, generator_quantum_static)\n",
    "        fake_imgs_intermediate = jax.vmap(generator_quantum, in_axes=0, out_axes=0)(z)\n",
    "        generator_linear = eqx.combine(generator_linear_params, generator_linear_static)\n",
    "        fake_imgs = jax.vmap(generator_linear, in_axes=0, out_axes=0)(fake_imgs_intermediate)\n",
    "        \n",
    "        return fake_imgs\n",
    "\n",
    "    # General real samples to evaluate the model\n",
    "    @eqx.filter_jit\n",
    "    def evaluate_real(key):\n",
    "        z= jax.random.uniform(key, shape=(config.eval_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)  # now a full-size sample, not just a batch.\n",
    "        real_imgs_intermediate = jax.vmap(generator_quantum_real, in_axes=0, out_axes=0)(z)\n",
    "        real_imgs = jax.vmap(generator_linear_real, in_axes=0, out_axes=0)(real_imgs_intermediate)\n",
    "        \n",
    "        return real_imgs\n",
    "\n",
    "    \n",
    "    ################################################### make experiment folder #######################################################\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime('%d_%m_%Y_%H_%M_%S')\n",
    "    current_folder = os.path.abspath(os.getcwd()) +'/'    \n",
    "\n",
    "    exp_folder = current_folder + '_' + timestamp + '_' + str(seed_initial) + '/'\n",
    "    os.makedirs(exp_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    ################################################### prepare training set #########################################################\n",
    "    # prepare training set and data loader\n",
    "    key = jax.random.PRNGKey(seed=seed_initial)\n",
    "    key, key_gq, key_gl, key_c, key_loader = jax.random.split(key, 5)\n",
    "    mnist_dataset = np.load(training_data_path)\n",
    "    dataloader = dataloader(jnp.array(mnist_dataset), batch_size=config.batch_size, key=key_loader) \n",
    "\n",
    "    ################################################### initialize models #########################################################\n",
    "    # quantum parameters are uniformly distributed\n",
    "    theta = jax.random.uniform(key_gq, shape=(config.nl, 2*config.nq-1), minval=-jnp.pi, maxval=jnp.pi)\n",
    "    generator_quantum = GeneratorQuantum(nq=config.nq, nl=config.nl, k=config.k, weights=theta)\n",
    "    \n",
    "    # generator parameters are initialised by default (Kaiming uniform)\n",
    "    n_obs = len(get_all_k_local_observables(config.nq, config.k))\n",
    "    generator_linear = GeneratorLinear(n_obs=n_obs, data_dim=config.data_dim, key=key_gl)\n",
    "    \n",
    "    # critic parameters are also initialised by default (Kaiming uniform)\n",
    "    critic = Critic(data_dim=config.data_dim, key=key_c)\n",
    "    \n",
    "    generator_quantum_params, generator_quantum_static = eqx.partition(generator_quantum, eqx.is_array)\n",
    "    generator_linear_params, generator_linear_static = eqx.partition(generator_linear, eqx.is_array)\n",
    "    critic_params, critic_static = eqx.partition(critic, eqx.is_array)\n",
    "\n",
    "    ################################################### initialize optimizers #########################################################\n",
    "    tx_gq = optax.adam(learning_rate=config.lr_gq, b1=config.b1_gq, b2=config.b2_gq)\n",
    "    tx_gl = optax.adam(learning_rate=config.lr_gl, b1=config.b1_gl, b2=config.b2_gl)\n",
    "    tx_c = optax.adam(learning_rate=config.lr_c, b1=config.b1_c, b2=config.b2_c)\n",
    "\n",
    "    generator_quantum_opt_state = tx_gq.init(generator_quantum_params)\n",
    "    generator_linear_opt_state = tx_gl.init(generator_linear_params)\n",
    "    critic_opt_state = tx_c.init(critic_params)\n",
    "    \n",
    "    loss_history = [] # tuples of (lg, lc, kld, quantum_params_dist, linear_params_dist)\n",
    "    best = np.infty\n",
    "    best_i = 0\n",
    "\n",
    "    ################################################### train loop starts here #########################################################\n",
    "    for i in tqdm(range(config.n_iter + 1)):\n",
    "        key, subkey, subsubkey = jax.random.split(key, 3)\n",
    "        generator_quantum_params, generator_linear_params, critic_params, generator_quantum_opt_state, generator_linear_opt_state, critic_opt_state, loss_generator_quantum, loss_generator_linear, loss_critic, key = train_step(generator_quantum_params, generator_linear_params, critic_params, generator_quantum_opt_state, generator_linear_opt_state, critic_opt_state, key)\n",
    "\n",
    "        # Evaluation\n",
    "        if i % config.eval_freq == 0:\n",
    "            fake_imgs = evaluate_fake(generator_quantum_params, generator_linear_params, subkey)\n",
    "            real_imgs = mnist_dataset[0:config.eval_size]\n",
    "\n",
    "            kld = kld_estimator(fake_imgs, real_imgs)  # here k is used for KNN estimator. \n",
    "            loss_history.append((loss_generator_quantum.item(), loss_critic.item(), kld))\n",
    "\n",
    "        if i in [0,1,2,5,10,20,50,100,200,500,1000,2000,5000,10000,20000,50000]:\n",
    "            eqx.tree_serialise_leaves(exp_folder + str(i) + \"_generator_quantum.eqx\", copy.deepcopy(generator_quantum_params))\n",
    "            eqx.tree_serialise_leaves(exp_folder + str(i) + \"_generator_linear.eqx\", copy.deepcopy(generator_linear_params))\n",
    "\n",
    "        elif kld < best:\n",
    "            eqx.tree_serialise_leaves(exp_folder + str(i) + \"_generator_quantum.eqx\", copy.deepcopy(generator_quantum_params))\n",
    "            eqx.tree_serialise_leaves(exp_folder + str(i) + \"_generator_linear.eqx\", copy.deepcopy(generator_linear_params))\n",
    "            best = kld\n",
    "            best_i = i\n",
    "\n",
    "    loss_history = np.array(loss_history)\n",
    "    np.save(exp_folder+\"loss_history.npy\", np.array(loss_history))\n",
    "\n",
    "    # plot loss curves\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "    ax.plot(np.arange(len(loss_history)) * config.eval_freq, loss_history[:,0], label='Critic Loss')\n",
    "    ax.plot(np.arange(len(loss_history)) * config.eval_freq, loss_history[:,1], label='Generator Loss')\n",
    "    ax.plot(np.arange(len(loss_history)) * config.eval_freq, loss_history[:,2], label='KLD')\n",
    "    ax.set_xlabel('Updates of Quantum Parameters')\n",
    "    ax.set_ylabel('Metrics')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(exp_folder+\"training_curves.png\")\n",
    "    plt.close()\n",
    "\n",
    "    best_generator_quantum_params = eqx.tree_deserialise_leaves(exp_folder + str(best_i) + \"_generator_quantum.eqx\", generator_quantum_params)\n",
    "    best_generator_quantum = eqx.combine(best_generator_quantum_params, generator_quantum_static)\n",
    "    best_generator_linear_params = eqx.tree_deserialise_leaves(exp_folder + str(best_i) + \"_generator_linear.eqx\", generator_linear_params)\n",
    "    best_generator_linear = eqx.combine(best_generator_linear_params, generator_linear_static)\n",
    "\n",
    "    return best_generator_quantum, best_generator_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e26be-2d68-453f-b98a-83b1b8beeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_quantum, generator_linear = train(config=config, training_data_path = f'data/ae_fashion.npy', seed_initial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651bddb3-36b1-49f3-94e9-d4c25c23e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(eqx.Module):\n",
    "    encoder: eqx.Module\n",
    "    decoder: eqx.Module\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2 = jax.random.split(key)\n",
    "        # Encoder: 784 -> 128 -> 64 -> latent_dim\n",
    "        self.encoder = eqx.nn.Sequential([\n",
    "            eqx.nn.Linear(config.original_data_dim, 128, key=key1),\n",
    "            eqx.nn.Lambda(jax.nn.relu),\n",
    "            eqx.nn.Linear(128, 64, key=key2),\n",
    "            eqx.nn.Lambda(jax.nn.relu),\n",
    "            eqx.nn.Linear(64, config.data_dim, key=key1),  # Bottleneck\n",
    "        ])\n",
    "        # Decoder: latent_dim -> 64 -> 128 -> 784\n",
    "        self.decoder = eqx.nn.Sequential([\n",
    "            eqx.nn.Linear(config.data_dim, 64, key=key2),\n",
    "            eqx.nn.Lambda(jax.nn.relu),\n",
    "            eqx.nn.Linear(64, 128, key=key1),\n",
    "            eqx.nn.Lambda(jax.nn.relu),\n",
    "            eqx.nn.Linear(128, config.original_data_dim, key=key2),\n",
    "            eqx.nn.Lambda(jax.nn.sigmoid),  # MNIST pixels are in [0, 1]\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "        \n",
    "autoencoder_pretrained = eqx.tree_deserialise_leaves('data/ae_fashion.eqx', Autoencoder(jax.random.PRNGKey(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77e613-f68a-490c-8187-90efb2734bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples(generator_quantum, generator_linear, key):\n",
    "    z= jax.random.uniform(key, shape=(config.eval_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)  # now a full-size sample, not just a batch.\n",
    "    fake_imgs_intermediate = jax.vmap(generator_quantum, in_axes=0, out_axes=0)(z)\n",
    "    fake_imgs = jax.vmap(generator_linear, in_axes=0, out_axes=0)(fake_imgs_intermediate)\n",
    "    return fake_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07779301-f2b2-4b8e-a06d-dc42c17dac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show(imgs, save_name):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.savefig(save_name + '.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7a1d6-0b49-4dad-8e25-1caef46439f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "key, subkey, subsubkey = jax.random.split(key, 3)\n",
    "fake_images_compressed = make_samples(generator_quantum, generator_linear, subkey)\n",
    "\n",
    "fake_images = jax.vmap(autoencoder_pretrained.decoder)(fake_images_compressed[0:25]).reshape((25, 1, 28, 28))\n",
    "\n",
    "show_grid = make_grid(torch.tensor(np.array(fake_images)), nrow=5)\n",
    "show(show_grid, \"shadow_4\")\n",
    "\n",
    "mnist_latent_training = np.load(f'data/ae_fashion.npy')[0:2048] # Max allowed samples\n",
    "\n",
    "kld_estimate = kld_estimator(fake_images_compressed, mnist_latent_training)\n",
    "\n",
    "print(f\"OT Shadow KLD: {kld_estimate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856b320-9dd7-41b6-9704-e3d7d8e90b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9a62a-208b-4abc-a546-8da37c968f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otevs_kernel",
   "language": "python",
   "name": "otevs_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
