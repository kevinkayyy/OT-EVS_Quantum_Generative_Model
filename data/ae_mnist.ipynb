{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674e8819-5980-4343-bb8f-d586a1e73e7e",
   "metadata": {},
   "source": [
    "# This notebook is for training a new autoencoder to downscale the MNIST (or Fashion-MNIST) dataset.\n",
    "### To download the MNIST dataset, you need to install pytorch or find an equivalent source.\n",
    "### Otherwise, you may use our pretrained autoencoder and downscaled data in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6c205-3d1e-4151-9499-dc6d48b99b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import equinox as eqx\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 8  # Size of the bottleneck (compressed representation)\n",
    "#latent_dim = 16 # for Fashion-MNIST, it's better to use higher dimensions\n",
    "input_dim = 784  # MNIST is 28x28 = 784\n",
    "learning_rate = 1e-3\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "\n",
    "# Load MNIST\n",
    "def numpy_collate(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return np.stack(images), np.stack(labels)\n",
    "\n",
    "train_dataset = MNIST(root=\"raw_mnist/\", train=True, download=True, transform=lambda x: np.array(x, dtype=np.float32).reshape(-1) / 255.0)\n",
    "#train_dataset = FashionMNIST(root=\"raw_mnist/\", train=True, download=True, transform=lambda x: np.array(x, dtype=np.float32).reshape(-1) / 255.0)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
    "latent_loader = DataLoader(train_dataset, batch_size=80, shuffle=False, collate_fn=numpy_collate)\n",
    "evaluation_loader = DataLoader(train_dataset, batch_size=96, shuffle=True, collate_fn=numpy_collate)\n",
    "\n",
    "# Define Autoencoder\n",
    "class Autoencoder(eqx.Module):\n",
    "    encoder: eqx.Module\n",
    "    decoder: eqx.Module\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2 = jax.random.split(key)\n",
    "        # Encoder: 784 -> 128 -> 64 -> latent_dim\n",
    "        self.encoder = eqx.nn.Sequential([\n",
    "            eqx.nn.Linear(input_dim, 128, key=key1),\n",
    "            eqx.nn.Lambda(jax.nn.relu),\n",
    "            eqx.nn.Linear(128, 64, key=key2),\n",
    "            eqx.nn.Lambda(jax.nn.relu),\n",
    "            eqx.nn.Linear(64, latent_dim, key=key1),  # Bottleneck\n",
    "        ])\n",
    "        # Decoder: latent_dim -> 64 -> 128 -> 784\n",
    "        self.decoder = eqx.nn.Sequential([\n",
    "            eqx.nn.Linear(latent_dim, 64, key=key2),\n",
    "            eqx.nn.Lambda(jax.nn.relu),\n",
    "            eqx.nn.Linear(64, 128, key=key1),\n",
    "            eqx.nn.Lambda(jax.nn.relu),\n",
    "            eqx.nn.Linear(128, input_dim, key=key2),\n",
    "            eqx.nn.Lambda(jax.nn.sigmoid),  # MNIST pixels are in [0, 1]\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba608f4-b6b5-4f58-8565-56e60c0492b2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4d379-b248-4a1f-90f2-8c7a286ce011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "key = jax.random.PRNGKey(42)\n",
    "model = Autoencoder(key)\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "# Loss function (MSE)\n",
    "@eqx.filter_value_and_grad\n",
    "def compute_loss(model, x):\n",
    "    x_recon = jax.vmap(model)(x)  # Vectorize over batch\n",
    "    return jnp.mean((x_recon - x) ** 2)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def train_step(model, opt_state, x):\n",
    "    loss, grads = compute_loss(model, x)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        x, _ = batch\n",
    "        x = jnp.array(x)  # Convert to JAX array\n",
    "        model, opt_state, loss = train_step(model, opt_state, x)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "eqx.tree_serialise_leaves(\"autoencoder_mnist_8.eqx\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fc0fc-76ce-4638-8370-e229daad3229",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30a2b9-7bf5-4aa6-94d5-99f2407dfb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "for batch in evaluation_loader:\n",
    "    x, _ = batch\n",
    "    x = jnp.array(x)  # Convert to JAX array\n",
    "    x_recon = jax.vmap(model)(x)\n",
    "    #model, opt_state, loss = train_step(model, opt_state, x)\n",
    "    break\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "x_recon = torch.tensor(np.array(x_recon))\n",
    "x_recon = x_recon.view(96, 1, 28, 28)\n",
    "\n",
    "show(make_grid(x_recon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad370343-8f01-41b3-bd9a-88177de1180a",
   "metadata": {},
   "source": [
    "## Extract latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10a9e29-4396-4506-b7dd-6b7eeb3c2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "model = Autoencoder(key)\n",
    "model = eqx.tree_deserialise_leaves(\"autoencoder_mnist_8.eqx\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31a8bf-9ba8-4ac8-82d6-d32f4e5c2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space = []\n",
    "labels = [] \n",
    "for batch in latent_loader:\n",
    "    x, label = batch\n",
    "    x = jnp.array(x)  # Convert to JAX array\n",
    "    z = jax.vmap(model.encoder)(x)\n",
    "    latent_space.append(np.array(z))\n",
    "    labels.append(label)\n",
    "\n",
    "latent_space = jnp.asarray(latent_space)\n",
    "latent_vector = jnp.reshape(latent_space, (60000, latent_dim))\n",
    "\n",
    "np.save(\"ae_mnist_8\", latent_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7695ab9-677d-45c0-82a4-9023fa27351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in evaluation_loader:\n",
    "    x, _ = batch\n",
    "    x = jnp.array(x)  # Convert to JAX array\n",
    "    x_latent = jax.vmap(model.encoder)(x)\n",
    "    break\n",
    "\n",
    "x_latent = np.load(\"ae_mnist_8.npy\")\n",
    "\n",
    "plt.imshow(x_latent[:20, :])\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otevs_kernel",
   "language": "python",
   "name": "otevs_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
