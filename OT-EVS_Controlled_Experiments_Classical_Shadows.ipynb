{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b3e943-b8db-42b4-8a9a-7b5452da26fc",
   "metadata": {},
   "source": [
    "# Demonstration of the Observable-Tunable Expectation Value Sampler Quantum Generative Model (OT-EVS) on the controlled experiments with synthetic datasets\n",
    "### This notebook produces similar results as in Sections V.A-C of the article \"Shadow-Frugal Expectation-Value-Sampling Variational Quantum Generative Model\" (arXiv:2412.17039).\n",
    "### Below we demonstrate the training of OT-EVS with the classical shadows measurements. Users may check the other notebooks which use conventional measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363a7cc-5b00-4c7f-bb31-855bf20cdfcf",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4a92d6-0051-4d27-b708-96f58ed948d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from itertools import product\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import PRNGKeyArray\n",
    "from jax.tree_util import tree_map\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import tensorcircuit as tc\n",
    "\n",
    "import faiss\n",
    "from scipy.special import digamma\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fc846-f996-4d31-8a73-632e43bb5627",
   "metadata": {},
   "source": [
    "## Experiment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc009c8-40f2-4514-8f1a-1cece2e56ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # critic architecture\n",
    "    'critic_layer_size': 512,   # width of MLP hidden layers in the critic\n",
    "    'critic_depth': 4,  # depth of MLP hidden layers in the critic\n",
    "    'n_critic': 5,  # how many times to update the critic before updating the generator\n",
    "\n",
    "    # generator architecture\n",
    "    'latent_dim': 2,   # dimension of input Gaussian random variables\n",
    "    'data_dim': 2,  # dimension of output \n",
    "    'nq': 4,  # number of qubits\n",
    "    'nl': 2,   # number of circuit layers\n",
    "    'k': 1,   # locality of observables\n",
    "    'n_shots': 4096,   # number of shots per observable\n",
    "\n",
    "    # learning and decay rates for the generator (circuit part)\n",
    "    'lr_gq': 0.001, \n",
    "    'b1_gq': 0,\n",
    "    'b2_gq': 0.9,\n",
    "\n",
    "    # learning and decay rates for the generator (observable part)\n",
    "    'lr_gl': 0.0001, \n",
    "    'b1_gl': 0.9,\n",
    "    'b2_gl': 0.9,\n",
    "\n",
    "    # learning and decay rates for the critic\n",
    "    'lr_c': 0.0001,\n",
    "    'b1_c': 0.5,\n",
    "    'b2_c': 0.9,\n",
    "\n",
    "    'lambda_gp': 0.1,  # scalar in front of the gradient penalty term \n",
    "    'batch_size': 256,  # batch size\n",
    "\n",
    "    'n_iter': 20000,  # how many training iterations to use\n",
    "    'eval_freq': 200,   # how often to estimate the KLD\n",
    "    'train_size': 65536,   # how many data to include in the training set\n",
    "    'eval_size': 2048   # how many samples and training data to use to estimate the KLD\n",
    "}\n",
    "\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481ab15-922b-434d-ba25-d79856010e71",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb7b16a-dbeb-4527-8495-1fa36427e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tc.set_backend('jax')\n",
    "\n",
    "def get_all_k_local_observables(nq, k):\n",
    "    '''\n",
    "    The observables\n",
    "    '''\n",
    "    all_tuples = product([0, 1, 2, 3], repeat=nq)\n",
    "    valid_tuples = [t for t in all_tuples if (sum(1 for x in t if x == 0) >= nq - k and sum(1 for x in t if x == 0) < nq)]\n",
    "    \n",
    "    return jnp.array(valid_tuples)\n",
    "\n",
    "\n",
    "def get_circuit(nq, nl, inputs, weights):\n",
    "    '''\n",
    "    The circuit\n",
    "    '''\n",
    "    circuit = tc.Circuit(nq)\n",
    "    for l in range(nl):\n",
    "        for i in range(nq):\n",
    "            circuit.rx(i, theta=inputs[l])\n",
    "            circuit.ry(i, theta=weights[l,i])\n",
    "        for i in range(0,nq-1):\n",
    "            circuit.cnot(i, i+1)\n",
    "            circuit.ry(i+1, theta=weights[l, nq+i])\n",
    "            circuit.cnot(i, i+1)\n",
    "    \n",
    "    return circuit \n",
    "\n",
    "\n",
    "class GeneratorQuantum(eqx.Module):\n",
    "    nq: int = eqx.field(static=True)\n",
    "    nl: int = eqx.field(static=True)\n",
    "    k: int = eqx.field(static=True)\n",
    "    weights: jax.Array\n",
    "    \n",
    "    @K.jit\n",
    "    def evaluate_circuit(self, inputs, observable):\n",
    "        circuit = get_circuit(self.nq, self.nl, inputs, self.weights)\n",
    "        return tc.templates.measurements.parameterized_measurements(circuit, observable, onehot=True)\n",
    "\n",
    "    ### Call this function when using the classical shadows method\n",
    "    def get_2k_values(self, x):\n",
    "        all_2k_observables = get_all_k_local_observables(self.nq, int(min(2 * self.k, self.nq)))\n",
    "        return K.vmap(self.evaluate_circuit, vectorized_argnums=1)(x, all_2k_observables)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        all_observables = get_all_k_local_observables(self.nq, self.k)\n",
    "\n",
    "        return K.vmap(self.evaluate_circuit, vectorized_argnums=1)(x, all_observables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7678d6dd-e986-4848-9e8f-d2b6c319b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorLinear(eqx.Module):\n",
    "    ''' \n",
    "    The generator (observable part)\n",
    "    '''\n",
    "    model: eqx.Module\n",
    "    \n",
    "    def __init__(self, n_obs, data_dim, key):\n",
    "        super(GeneratorLinear, self).__init__()\n",
    "        \n",
    "        self.model = eqx.nn.Linear(n_obs, data_dim, key=key)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c332e0b-1fe7-4cfd-b304-ecd11e1caeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(eqx.Module):\n",
    "    '''\n",
    "    The critic\n",
    "    '''\n",
    "    layers:list\n",
    "    \n",
    "    def __init__(self, data_dim, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(data_dim, 512, key=key1), \n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(512, 512, key=key2),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(512, 512, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(512, 1, key=key4),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6efd3-197f-479c-9bf8-f456779662f7",
   "metadata": {},
   "source": [
    "## KLD Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943397ab-9bab-4ed6-9b82-ffcd0a6270c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kld_estimator(s1, s2):\n",
    "    # equation 25 of the reference paper\n",
    "    s1, s2 = np.array(s1), np.array(s2)\n",
    "    n, m = len(s1), len(s2)\n",
    "    d = int(s1.shape[1])\n",
    "\n",
    "    #res = faiss.StandardGpuResources()\n",
    "\n",
    "    index_s1=faiss.IndexFlatL2(d)\n",
    "    #index_s1=faiss.index_cpu_to_gpu(res,0,index_s1)\n",
    "    index_s1.add(s1)\n",
    "\n",
    "    index_s2=faiss.IndexFlatL2(d)\n",
    "    #index_s2=faiss.index_cpu_to_gpu(res,0,index_s2)\n",
    "    index_s2.add(s2)\n",
    "\n",
    "    fulldist1 = np.sqrt(index_s1.search(s1, n)[0])\n",
    "    fulldist2 = np.sqrt(index_s2.search(s1, m)[0])\n",
    "\n",
    "    rhoi=fulldist1[::,1].reshape(-1)\n",
    "    nui=fulldist2[::,0].reshape(-1)\n",
    "\n",
    "    epsilon=np.maximum(rhoi, nui)\n",
    "    arg=np.where(rhoi>=nui, 0, 1)\n",
    "\n",
    "    li = np.array([np.searchsorted(fulldist1[i], epsilon[i], side='right') for i in range(m)]) - 1\n",
    "    ki = np.array([np.searchsorted(fulldist2[i], epsilon[i], side='right') for i in range(n)])\n",
    "\n",
    "\n",
    "    return np.mean(digamma(li)-digamma(ki)) + np.log(m / (n - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3525c-d9e2-4e80-b8d5-71b5db8c630e",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01cfc8d-62e9-46ec-b77b-f551be228545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, seed_data, seed_initial):\n",
    "    ############################# Data loader #####################################\n",
    "    def dataloader(data, batch_size, *, key):\n",
    "        dataset_size = data.shape[0]\n",
    "        indices = jnp.arange(dataset_size)\n",
    "        while True:\n",
    "            key, subkey = jax.random.split(key, 2)\n",
    "            perm = jax.random.permutation(subkey, indices)\n",
    "            start = 0\n",
    "            end = batch_size\n",
    "            while end < dataset_size:\n",
    "                batch_perm = perm[start:end]\n",
    "                yield data[batch_perm]\n",
    "                start = end\n",
    "                end = start + batch_size \n",
    "    # Yield a batch of training data from the training set\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from dataloader\n",
    "\n",
    "    ############################ Add shot noise (conventional method) to ideal outputs ############################\n",
    "    @jax.jit\n",
    "    def add_sampling_error(exact, n_shots, key):\n",
    "        p = jnp.clip((1 - exact) / 2, 0, 1)\n",
    "        mean = n_shots * p\n",
    "        std = jnp.sqrt(jnp.clip(n_shots * p * (1 - p), min=1e-16))\n",
    "        return 1 - 2 * jnp.clip((jax.random.normal(key) * std + mean) / n_shots, 0, 1)\n",
    "\n",
    "    ############################ Add shot noise (classical shadow method) to ideal outputs ############################\n",
    "    def index_and_factor_matrices(nq, k):\n",
    "        '''The first and second output entries are pauli basis and factor respectively. '''\n",
    "        rule_map = {(0, 0): (0, 1), (0, 1): (1, 1), (0, 2): (2, 1), (0, 3): (3, 1),\n",
    "                    (1, 0): (1, 1), (1, 1): (0, 3), (1, 2): (-1, 0), (1, 3): (-1, 0),\n",
    "                    (2, 0): (2, 1), (2, 1): (-1, 0), (2, 2): (0, 3), (2, 3): (-1, 0),\n",
    "                    (3, 0): (3, 1), (3, 1): (-1, 0), (3, 2): (-1, 0), (3, 3): (0, 3)}\n",
    "        \n",
    "        def rule(pauli_a, pauli_b):\n",
    "            '''Optimized rule lookup using pre-defined map.'''\n",
    "            return rule_map[(pauli_a, pauli_b)]\n",
    "        \n",
    "        obs_k = np.array(get_all_k_local_observables(nq, k))\n",
    "        obs_2k = np.array(get_all_k_local_observables(nq, min(2*k, nq)))\n",
    "        obs_2k = np.concatenate([np.zeros((1, nq), dtype=int), obs_2k], axis=0)\n",
    "        \n",
    "        num_k = len(obs_k)\n",
    "        \n",
    "        indices = np.zeros((num_k, num_k), dtype=int)\n",
    "        factors = np.zeros((num_k, num_k), dtype=int)\n",
    "        \n",
    "        obs_2k_dict = {tuple(row): idx for idx, row in enumerate(obs_2k)}\n",
    "        \n",
    "        for i in range(num_k):\n",
    "            for j in range(num_k):\n",
    "                output = np.array([rule(obs_k[i][q], obs_k[j][q]) for q in range(nq)])\n",
    "                \n",
    "                obs_out = tuple(output[:, 0])  # convert the array to a tuple for hashing\n",
    "                factor = np.prod(output[:, 1])\n",
    "                \n",
    "                indices[i][j] = obs_2k_dict.get(obs_out, 0)  # default to 0 if not found\n",
    "                factors[i][j] = factor\n",
    "    \n",
    "        return jnp.array(indices, dtype=int), jnp.array(factors, dtype=int)\n",
    "\n",
    "\n",
    "    def generate_covariance_matrix(obs_k, obs_2k, indices_matrix, factors_matrix):\n",
    "    \n",
    "        return obs_2k[indices_matrix] * factors_matrix - jnp.outer(obs_k, obs_k)\n",
    "    \n",
    "    \n",
    "    def add_shadow_error(mean, cov, n_shots, key):\n",
    "        L = jnp.linalg.cholesky(cov / n_shots)\n",
    "    \n",
    "        return mean + jnp.dot(jax.random.normal(key, shape=(1, mean.shape[0])), L.T)\n",
    "        \n",
    "    ###################################### One iteration during training  ######################################\n",
    "    @eqx.filter_jit\n",
    "    def train_step(generator_quantum_params, generator_linear_params, critic_params, generator_quantum_opt_state, generator_linear_opt_state, critic_opt_state, key):\n",
    "\n",
    "        # The block below contains subroutines  for a training step. \n",
    "        ############################### Classical Shadows Method ##############################\n",
    "        # Evaluate the generator (circuit part)\n",
    "        @eqx.filter_value_and_grad(has_aux=False)\n",
    "        def compute_grads_generator_quantum(generator_quantum_params, generator_linear_params, critic_params, z, keys):\n",
    "            generator_quantum = eqx.combine(generator_quantum_params, generator_quantum_static)\n",
    "            fake_batch_intermediate = jax.vmap(generator_quantum, in_axes=0, out_axes=0)(z)\n",
    "            obs_2k_batch = jax.vmap(generator_quantum.get_2k_values, in_axes=0, out_axes=0)(z)\n",
    "            obs_2k_batch = jnp.concatenate([jnp.ones((config.batch_size,1)), obs_2k_batch], axis=1)\n",
    "            cov_batch = jax.vmap(generate_covariance_matrix, in_axes=(0, 0, None, None))(fake_batch_intermediate, obs_2k_batch, indices_matrix, factors_matrix)\n",
    "            fake_batch_sampled = jnp.squeeze(jax.vmap(add_shadow_error, in_axes=(0, 0, None, 0))(fake_batch_intermediate, cov_batch, config.n_shots * n_obs, keys))\n",
    "            generator_linear = eqx.combine(generator_linear_params, generator_linear_static)\n",
    "            fake_batch = jax.vmap(generator_linear, in_axes=0, out_axes=0)(fake_batch_sampled)\n",
    "            critic = eqx.combine(critic_params, critic_static)\n",
    "            fake_value = jax.vmap(critic, in_axes=0, out_axes=0)(fake_batch)\n",
    "            loss = -fake_value.mean()\n",
    "\n",
    "            return loss\n",
    "        \n",
    "        # Evaluate the generator (observable part)\n",
    "        @eqx.filter_value_and_grad(has_aux=False)\n",
    "        def compute_grads_generator_linear(generator_linear_params, generator_quantum_params, critic_params, z, keys):\n",
    "            generator_quantum = eqx.combine(generator_quantum_params, generator_quantum_static)\n",
    "            fake_batch_intermediate = jax.vmap(generator_quantum, in_axes=0, out_axes=0)(z)\n",
    "            obs_2k_batch = jax.vmap(generator_quantum.get_2k_values, in_axes=0, out_axes=0)(z)\n",
    "            obs_2k_batch = jnp.concatenate([jnp.ones((config.batch_size,1)), obs_2k_batch], axis=1)\n",
    "            cov_batch = jax.vmap(generate_covariance_matrix, in_axes=(0, 0, None, None))(fake_batch_intermediate, obs_2k_batch, indices_matrix, factors_matrix)\n",
    "            fake_batch_sampled = jnp.squeeze(jax.vmap(add_shadow_error, in_axes=(0, 0, None, 0))(fake_batch_intermediate, cov_batch, config.n_shots * n_obs, keys))\n",
    "            generator_linear = eqx.combine(generator_linear_params, generator_linear_static)\n",
    "            fake_batch = jax.vmap(generator_linear, in_axes=0, out_axes=0)(fake_batch_sampled)\n",
    "            critic = eqx.combine(critic_params, critic_static)\n",
    "            fake_value = jax.vmap(critic, in_axes=0, out_axes=0)(fake_batch)\n",
    "            loss = -fake_value.mean()\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Subroutine for evaluating the critic\n",
    "        @eqx.filter_vmap(in_axes=(0, None))\n",
    "        @eqx.filter_grad(has_aux=False)\n",
    "        def critic_forward(input_data, critic):\n",
    "            \"\"\"Helper function to calculate the gradients with respect to the input.\"\"\"\n",
    "            value = critic(input_data)\n",
    "            return value[0]\n",
    "\n",
    "        # Evaluate the critic\n",
    "        @eqx.filter_value_and_grad(has_aux=False)\n",
    "        def compute_grads_critic(critic_params, generator_quantum_params, generator_linear_params, real_batch, z, key, keys):\n",
    "            generator_quantum = eqx.combine(generator_quantum_params, generator_quantum_static)\n",
    "            fake_batch_intermediate = jax.vmap(generator_quantum, in_axes=0, out_axes=0)(z)\n",
    "            obs_2k_batch = jax.vmap(generator_quantum.get_2k_values, in_axes=0, out_axes=0)(z)\n",
    "            obs_2k_batch = jnp.concatenate([jnp.ones((config.batch_size,1)), obs_2k_batch], axis=1)\n",
    "            cov_batch = jax.vmap(generate_covariance_matrix, in_axes=(0, 0, None, None))(fake_batch_intermediate, obs_2k_batch, indices_matrix, factors_matrix)\n",
    "            fake_batch_sampled = jnp.squeeze(jax.vmap(add_shadow_error, in_axes=(0, 0, None, 0))(fake_batch_intermediate, cov_batch, config.n_shots * n_obs, keys))\n",
    "            generator_linear = eqx.combine(generator_linear_params, generator_linear_static)\n",
    "            fake_batch = jax.vmap(generator_linear, in_axes=0, out_axes=0)(fake_batch_sampled)\n",
    "            critic = eqx.combine(critic_params, critic_static)\n",
    "            fake_value = jax.vmap(critic, in_axes=0, out_axes=0)(fake_batch)\n",
    "            real_value = jax.vmap(critic, in_axes=0, out_axes=0)(real_batch)\n",
    "            \n",
    "            epsilon = jax.random.uniform(key, shape=(config.batch_size, 1), minval=0, maxval=1)\n",
    "            data_mix = real_batch * epsilon + fake_batch * (1 - epsilon) \n",
    "            \n",
    "            grads = critic_forward(data_mix, critic)\n",
    "            grad_norm = jnp.linalg.norm(grads, axis=1)\n",
    "            gradient_penalty = jnp.mean((grad_norm - 1) ** 2)\n",
    "            \n",
    "            loss = -jnp.mean(real_value) + jnp.mean(fake_value) + config.lambda_gp * gradient_penalty\n",
    "\n",
    "            return loss\n",
    "\n",
    "            \n",
    "        ### The block below is what differs for the three training algorithms. Comment and Uncomment the blocks to switch algorithms\n",
    "\n",
    "        ########################################### Asynchronous version ########################################################\n",
    "        '''\n",
    "        for _, real_batch in zip(range(config.n_critic), infinite_trainloader()):\n",
    "            key, subkey, subsubkey, subsubsubkey = jax.random.split(key, 4)\n",
    "            z = jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "            keys = jax.random.split(subsubsubkey, config.batch_size).reshape(config.batch_size, 2)\n",
    "            \n",
    "            loss_critic, grads = compute_grads_critic(critic_params, generator_quantum_params, generator_linear_params, real_batch, z, subsubkey, keys)\n",
    "            updates, critic_opt_state = tx_c.update(grads, critic_opt_state)\n",
    "            critic_params = eqx.apply_updates(critic_params, updates)            \n",
    "        \n",
    "        key, subkey, subsubkey = jax.random.split(key, 3)\n",
    "        z= jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "        keys = jax.random.split(subsubkey, config.batch_size).reshape(config.batch_size, 2)\n",
    "        \n",
    "        for _ in range(config.n_critic):\n",
    "            loss_generator_linear, grads = compute_grads_generator_linear(generator_linear_params, generator_quantum_params, critic_params, z, keys)\n",
    "            updates, generator_linear_opt_state = tx_gl.update(grads, generator_linear_opt_state)\n",
    "            generator_linear_params = eqx.apply_updates(generator_linear_params, updates) \n",
    "        \n",
    "        loss_generator_quantum, grads = compute_grads_generator_quantum(generator_quantum_params, generator_linear_params, critic_params, z, keys)\n",
    "        updates, generator_quantum_opt_state = tx_gq.update(grads, generator_quantum_opt_state)\n",
    "        generator_quantum_params = eqx.apply_updates(generator_quantum_params, updates) \n",
    "        '''\n",
    "        ########################################### Decoupled version ##########################################################\n",
    "        '''\n",
    "        for _, real_batch in zip(range(config.n_critic), infinite_trainloader()):\n",
    "            key, subkey, subsubkey, subsubsubkey = jax.random.split(key, 4)\n",
    "            z = jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "            keys = jax.random.split(subsubsubkey, config.batch_size).reshape(config.batch_size, 2)\n",
    "            \n",
    "            loss_critic, grads = compute_grads_critic(critic_params, generator_quantum_params, generator_linear_params, real_batch, z, subsubkey, keys)\n",
    "            updates, critic_opt_state = tx_c.update(grads, critic_opt_state)\n",
    "            critic_params = eqx.apply_updates(critic_params, updates)            \n",
    "\n",
    "            loss_generator_linear, grads = compute_grads_generator_linear(generator_linear_params, generator_quantum_params, critic_params, z, keys)\n",
    "            updates, generator_linear_opt_state = tx_gl.update(grads, generator_linear_opt_state)\n",
    "            generator_linear_params = eqx.apply_updates(generator_linear_params, updates) \n",
    "        \n",
    "        key, subkey, subsubkey = jax.random.split(key, 3)\n",
    "        z= jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "        keys = jax.random.split(subsubkey, config.batch_size).reshape(config.batch_size, 2)\n",
    "        \n",
    "        loss_generator_quantum, grads = compute_grads_generator_quantum(generator_quantum_params, generator_linear_params, critic_params, z, keys)\n",
    "        updates, generator_quantum_opt_state = tx_gq.update(grads, generator_quantum_opt_state)\n",
    "        generator_quantum_params = eqx.apply_updates(generator_quantum_params, updates) \n",
    "        '''\n",
    "\n",
    "        ########################################### Joint version ##############################################################  \n",
    "\n",
    "        for _, real_batch in zip(range(config.n_critic), infinite_trainloader()):\n",
    "            key, subkey, subsubkey, subsubsubkey = jax.random.split(key, 4)\n",
    "            z = jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "            keys = jax.random.split(subsubsubkey, config.batch_size).reshape(config.batch_size, 2)\n",
    "            \n",
    "            loss_critic, grads = compute_grads_critic(critic_params, generator_quantum_params, generator_linear_params, real_batch, z, subsubkey, keys)\n",
    "            updates, critic_opt_state = tx_c.update(grads, critic_opt_state)\n",
    "            critic_params = eqx.apply_updates(critic_params, updates)            \n",
    "        \n",
    "        key, subkey, subsubkey = jax.random.split(key, 3)\n",
    "        z= jax.random.uniform(subkey, shape=(config.batch_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "        keys = jax.random.split(subsubkey, config.batch_size).reshape(config.batch_size, 2)\n",
    "        loss_generator_linear, grads_linear = compute_grads_generator_linear(generator_linear_params, generator_quantum_params, critic_params, z, keys)\n",
    "        loss_generator_quantum, grads_quantum = compute_grads_generator_quantum(generator_quantum_params, generator_linear_params, critic_params, z, keys)\n",
    "        updates_linear, generator_linear_opt_state = tx_gl.update(grads_linear, generator_linear_opt_state)\n",
    "        generator_linear_params = eqx.apply_updates(generator_linear_params, updates_linear) \n",
    "        updates_quantum, generator_quantum_opt_state = tx_gq.update(grads_quantum, generator_quantum_opt_state)\n",
    "        generator_quantum_params = eqx.apply_updates(generator_quantum_params, updates_quantum)         \n",
    "        \n",
    "        return generator_quantum_params, generator_linear_params, critic_params, generator_quantum_opt_state, generator_linear_opt_state, critic_opt_state, loss_generator_quantum, loss_generator_linear, loss_critic, key\n",
    "                \n",
    "\n",
    "    # Generate fake samples to evaluate the model\n",
    "    @eqx.filter_jit\n",
    "    def evaluate_fake(generator_quantum_params, generator_linear_params, key):\n",
    "        z = jax.random.uniform(key, shape=(config.eval_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)  # now a full-size sample, not just a batch.\n",
    "        \n",
    "        generator_quantum = eqx.combine(generator_quantum_params, generator_quantum_static)\n",
    "        fake_imgs_intermediate = jax.vmap(generator_quantum, in_axes=0, out_axes=0)(z)\n",
    "        generator_linear = eqx.combine(generator_linear_params, generator_linear_static)\n",
    "        fake_imgs = jax.vmap(generator_linear, in_axes=0, out_axes=0)(fake_imgs_intermediate)\n",
    "        \n",
    "        return fake_imgs\n",
    "\n",
    "    # General real samples to evaluate the model\n",
    "    @eqx.filter_jit\n",
    "    def evaluate_real(key):\n",
    "        z= jax.random.uniform(key, shape=(config.eval_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)  # now a full-size sample, not just a batch.\n",
    "        real_imgs_intermediate = jax.vmap(generator_quantum_real, in_axes=0, out_axes=0)(z)\n",
    "        real_imgs = jax.vmap(generator_linear_real, in_axes=0, out_axes=0)(real_imgs_intermediate)\n",
    "        \n",
    "        return real_imgs\n",
    "\n",
    "    \n",
    "    ################################################### make experiment folder #######################################################\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime('%d_%m_%Y_%H_%M_%S')\n",
    "    current_folder = os.path.abspath(os.getcwd()) +'/'    \n",
    "\n",
    "    exp_folder = current_folder + '_' + timestamp + '_' + '_' + str(seed_data) + '_' + str(seed_initial) + '/'\n",
    "    os.makedirs(exp_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    ################################################### prepare training set #########################################################\n",
    "    # set up keys\n",
    "    key = jax.random.PRNGKey(seed=seed_data)\n",
    "    key, key_real = jax.random.split(key, 2)    # key is used once to initialise the model and then split recursively during training, key_real is used once to initialise real data generator.\n",
    "\n",
    "    ### The blocks below are for preparing real data generator \n",
    "    key_gq, key_gq2, key_gl, key_gl2, key_loader = jax.random.split(key_real, 5)\n",
    "\n",
    "    # here quantum params are oriented\n",
    "    theta = jax.random.normal(key_gq, shape=(config.nl, 2*config.nq-1)) * np.pi / 8 + jax.random.uniform(key_gq2, minval=-1, maxval=1) * np.pi \n",
    "    generator_quantum_real = GeneratorQuantum(nq=config.nq, nl=config.nl, k=config.k, weights=theta)   # create model with initialised weights\n",
    "\n",
    "    # here observable weights are chosen sparse, bias are 0.         \n",
    "    n_obs = len(get_all_k_local_observables(config.nq, config.k))\n",
    "    generator_linear_1 = GeneratorLinear(n_obs=n_obs, data_dim=config.data_dim, key=key_gl)    # create model\n",
    "    bias = jnp.zeros(config.data_dim)\n",
    "    get_bias = lambda m: m.model.bias\n",
    "    generator_linear_2 = eqx.tree_at(get_bias, generator_linear_1, bias)    # initialise bias\n",
    "\n",
    "    def place_values_in_row(row_key):\n",
    "        idxs = jax.random.choice(row_key, n_obs, shape=(3,), replace=False)\n",
    "        return jnp.zeros(n_obs).at[idxs].set(jnp.array([1, 4, 9]))\n",
    "\n",
    "    keys = jax.random.split(key_gl2, config.data_dim)\n",
    "    weight_unnormalized = jnp.vstack([place_values_in_row(k) for k in keys])\n",
    "    weight = weight_unnormalized / jnp.linalg.norm(weight_unnormalized)\n",
    "    get_weight = lambda m: m.model.weight    \n",
    "    generator_linear_real = eqx.tree_at(get_weight, generator_linear_2, weight)    # initialise weight\n",
    "\n",
    "    generator_quantum_params_real, generator_quantum_static_real = eqx.partition(generator_quantum_real, eqx.is_array)\n",
    "    generator_linear_params_real, generator_linear_static_real = eqx.partition(generator_linear_real, eqx.is_array)\n",
    "\n",
    "    eqx.tree_serialise_leaves(exp_folder + \"generator_quantum_real.eqx\", copy.deepcopy(generator_quantum_params_real))\n",
    "    eqx.tree_serialise_leaves(exp_folder + \"generator_linear_real.eqx\", copy.deepcopy(generator_linear_params_real))\n",
    "\n",
    "    # prepare training set and data loader\n",
    "    z= jax.random.uniform(key, shape=(config.train_size, config.latent_dim), minval=-jnp.pi, maxval=jnp.pi)\n",
    "    dataset_intermediate = jax.vmap(generator_quantum_real, in_axes=0, out_axes=0)(z)\n",
    "    dataset = jax.vmap(generator_linear_real, in_axes=0, out_axes=0)(dataset_intermediate)\n",
    "    dataloader = dataloader(jnp.array(dataset), batch_size=config.batch_size, key=key_loader) \n",
    "\n",
    "\n",
    "    ################################################### initialize models #########################################################\n",
    "    key = jax.random.PRNGKey(seed=seed_initial)\n",
    "    key, key_gq, key_gl, key_c = jax.random.split(key, 4)\n",
    "\n",
    "    # quantum parameters are uniformly distributed\n",
    "    theta = jax.random.uniform(key_gq, shape=(config.nl, 2*config.nq-1), minval=-jnp.pi, maxval=jnp.pi)\n",
    "    generator_quantum = GeneratorQuantum(nq=config.nq, nl=config.nl, k=config.k, weights=theta)\n",
    "    \n",
    "    # generator parameters are initialised by default (Kaiming uniform)\n",
    "    generator_linear = GeneratorLinear(n_obs=n_obs, data_dim=config.data_dim, key=key_gl)\n",
    "    \n",
    "    # critic parameters are also initialised by default (Kaiming uniform)\n",
    "    critic = Critic(data_dim=config.data_dim, key=key_c)\n",
    "    \n",
    "    generator_quantum_params, generator_quantum_static = eqx.partition(generator_quantum, eqx.is_array)\n",
    "    generator_linear_params, generator_linear_static = eqx.partition(generator_linear, eqx.is_array)\n",
    "    critic_params, critic_static = eqx.partition(critic, eqx.is_array)\n",
    "    \n",
    "    indices_matrix, factors_matrix = index_and_factor_matrices(config.nq, config.k)\n",
    "\n",
    "    ################################################### initialize optimizers #########################################################\n",
    "    tx_gq = optax.adam(learning_rate=config.lr_gq, b1=config.b1_gq, b2=config.b2_gq)\n",
    "    tx_gl = optax.adam(learning_rate=config.lr_gl, b1=config.b1_gl, b2=config.b2_gl)\n",
    "    tx_c = optax.adam(learning_rate=config.lr_c, b1=config.b1_c, b2=config.b2_c)\n",
    "\n",
    "    generator_quantum_opt_state = tx_gq.init(generator_quantum_params)\n",
    "    generator_linear_opt_state = tx_gl.init(generator_linear_params)\n",
    "    critic_opt_state = tx_c.init(critic_params)\n",
    "    \n",
    "    loss_history = [] # tuples of (lg, lc, kld, quantum_params_dist, linear_params_dist)\n",
    "    best=np.infty\n",
    "\n",
    "    ################################################### train loop starts here #########################################################\n",
    "    for i in tqdm(range(config.n_iter + 1)):\n",
    "        key, subkey, subsubkey = jax.random.split(key, 3)\n",
    "        generator_quantum_params, generator_linear_params, critic_params, generator_quantum_opt_state, generator_linear_opt_state, critic_opt_state, loss_generator_quantum, loss_generator_linear, loss_critic, key = train_step(generator_quantum_params, generator_linear_params, critic_params, generator_quantum_opt_state, generator_linear_opt_state, critic_opt_state, key)\n",
    "\n",
    "        # Evaluation\n",
    "        if i % config.eval_freq == 0:\n",
    "            fake_imgs = evaluate_fake(generator_quantum_params, generator_linear_params, subkey)\n",
    "            real_imgs = evaluate_real(subsubkey)\n",
    "\n",
    "            kld = kld_estimator(fake_imgs, real_imgs)  # here k is used for KNN estimator. \n",
    "            loss_history.append((loss_generator_quantum.item(), loss_critic.item(), kld))\n",
    "\n",
    "        if i in [0,1,2,5,10,20,50,100,200,500,1000,2000,5000,10000,20000,50000]:\n",
    "            eqx.tree_serialise_leaves(exp_folder + str(i) + \"_generator_quantum.eqx\", copy.deepcopy(generator_quantum_params))\n",
    "            eqx.tree_serialise_leaves(exp_folder + str(i) + \"_generator_linear.eqx\", copy.deepcopy(generator_linear_params))\n",
    "\n",
    "        elif kld < best:\n",
    "            eqx.tree_serialise_leaves(exp_folder + str(i) + \"_generator_quantum.eqx\", copy.deepcopy(generator_quantum_params))\n",
    "            eqx.tree_serialise_leaves(exp_folder + str(i) + \"_generator_linear.eqx\", copy.deepcopy(generator_linear_params))\n",
    "            best = kld\n",
    "\n",
    "    loss_history = np.array(loss_history)\n",
    "    np.save(exp_folder+\"loss_history.npy\", np.array(loss_history))\n",
    "\n",
    "    # plot loss curves\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "    ax.plot(np.arange(len(loss_history)) * config.eval_freq, loss_history[:,0], label='Critic Loss')\n",
    "    ax.plot(np.arange(len(loss_history)) * config.eval_freq, loss_history[:,1], label='Generator Loss')\n",
    "    ax.plot(np.arange(len(loss_history)) * config.eval_freq, loss_history[:,2], label='KLD')\n",
    "    ax.set_xlabel('Updates of Quantum Parameters')\n",
    "    ax.set_ylabel('Metrics')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(exp_folder+\"training_curves.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e26be-2d68-453f-b98a-83b1b8beeb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/20001 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train(config=config, seed_data=6, seed_initial=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c66e3-5f0b-4eef-a84a-cda0b574976a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otevs_kernel",
   "language": "python",
   "name": "otevs_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
